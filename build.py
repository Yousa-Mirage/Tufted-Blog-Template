# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "feedgen",
# ]
# ///

"""
Tufted Blog Template æ„å»ºè„šæœ¬

è¿™æ˜¯ä¸€ä¸ªè·¨å¹³å°çš„æ„å»ºè„šæœ¬ï¼Œç”¨äºå°† Typst (.typ) æ–‡ä»¶ç¼–è¯‘ä¸º HTML å’Œ PDFï¼Œ
å¹¶å¤åˆ¶é™æ€èµ„æºåˆ°è¾“å‡ºç›®å½•ã€‚

æ”¯æŒå¢é‡ç¼–è¯‘ï¼šåªé‡æ–°ç¼–è¯‘ä¿®æ”¹åçš„æ–‡ä»¶ï¼ŒåŠ å¿«æ„å»ºé€Ÿåº¦ã€‚

ç”¨æ³•:
    uv run build.py build       # å®Œæ•´æ„å»º (HTML + PDF + èµ„æº)
    uv run build.py html        # ä»…æ„å»º HTML æ–‡ä»¶
    uv run build.py pdf         # ä»…æ„å»º PDF æ–‡ä»¶
    uv run build.py assets      # ä»…å¤åˆ¶é™æ€èµ„æº
    uv run build.py clean       # æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶
    uv run build.py preview     # å¯åŠ¨æœ¬åœ°é¢„è§ˆæœåŠ¡å™¨ï¼ˆé»˜è®¤ç«¯å£ 8000ï¼‰
    uv run build.py preview -p 3000  # ä½¿ç”¨è‡ªå®šä¹‰ç«¯å£
    uv run build.py --help      # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

å¢é‡ç¼–è¯‘é€‰é¡¹:
    --force, -f                 # å¼ºåˆ¶å®Œæ•´é‡å»ºï¼Œå¿½ç•¥å¢é‡æ£€æŸ¥

é¢„è§ˆæœåŠ¡å™¨é€‰é¡¹:
    --port, -p PORT             # æŒ‡å®šæœåŠ¡å™¨ç«¯å£å·ï¼ˆé»˜è®¤: 8000ï¼‰

ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ Python è¿è¡Œ:
    python build.py build
    python build.py build --force
    python build.py preview -p 3000
"""

import argparse
import html
import os
import re
import shutil
import subprocess
import sys
import threading
import time
import webbrowser
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path

from feedgen.feed import FeedGenerator
from feedgen.entry import FeedEntry

# ============================================================================
# é…ç½®
# ============================================================================

CONTENT_DIR = Path("content")  # æºæ–‡ä»¶ç›®å½•
SITE_DIR = Path("_site")  # è¾“å‡ºç›®å½•
ASSETS_DIR = Path("assets")  # é™æ€èµ„æºç›®å½•
CONFIG_FILE = Path("config.typ")  # å…¨å±€é…ç½®æ–‡ä»¶


@dataclass
class BuildStats:
    """æ„å»ºç»Ÿè®¡ä¿¡æ¯"""

    success: int = 0
    skipped: int = 0
    failed: int = 0

    def format_summary(self) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡æ‘˜è¦"""
        parts = []
        if self.success > 0:
            parts.append(f"ç¼–è¯‘: {self.success}")
        if self.skipped > 0:
            parts.append(f"è·³è¿‡: {self.skipped}")
        if self.failed > 0:
            parts.append(f"å¤±è´¥: {self.failed}")
        return ", ".join(parts) if parts else "æ— æ–‡ä»¶éœ€è¦å¤„ç†"

    @property
    def has_failures(self) -> bool:
        """æ˜¯å¦å­˜åœ¨å¤±è´¥"""
        return self.failed > 0


# ============================================================================
# å¢é‡ç¼–è¯‘è¾…åŠ©å‡½æ•°
# ============================================================================


def get_file_mtime(path: Path) -> float:
    """
    è·å–æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´æˆ³ã€‚

    å‚æ•°:
        path: æ–‡ä»¶è·¯å¾„

    è¿”å›:
        float: ä¿®æ”¹æ—¶é—´æˆ³ï¼Œæ–‡ä»¶ä¸å­˜åœ¨è¿”å› 0
    """
    try:
        return path.stat().st_mtime
    except (OSError, FileNotFoundError):
        return 0.0


def is_dep_file(path: Path) -> bool:
    """
    åˆ¤æ–­ä¸€ä¸ªæ–‡ä»¶æ˜¯å¦è¢«è¿½è¸ªä¸ºä¾èµ–ï¼‰ã€‚

    content/ ä¸‹çš„æ™®é€šé¡µé¢æ–‡ä»¶ä¸è¢«è§†ä¸ºæ¨¡æ¿æ–‡ä»¶ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ç‹¬ç«‹çš„é¡µé¢ï¼Œ
    ä¸åº”è¯¥ç›¸äº’ä¾èµ–ã€‚

    å‚æ•°:
        path: æ–‡ä»¶è·¯å¾„

    è¿”å›:
        bool: æ˜¯å¦æ˜¯ä¾èµ–æ–‡ä»¶
    """
    try:
        resolved_path = path.resolve()
        project_root = Path(__file__).parent.resolve()
        content_dir = (project_root / CONTENT_DIR).resolve()

        # config.typ æ˜¯ä¾èµ–æ–‡ä»¶
        if resolved_path == (project_root / CONFIG_FILE).resolve():
            return True

        # æ£€æŸ¥æ˜¯å¦åœ¨ content/ ç›®å½•ä¸‹
        try:
            relative_to_content = resolved_path.relative_to(content_dir)
            # content/_* ç›®å½•ä¸‹çš„æ–‡ä»¶è§†ä¸ºä¾èµ–æ–‡ä»¶
            parts = relative_to_content.parts
            if len(parts) > 0 and parts[0].startswith("_"):
                return True
            # content/ ä¸‹çš„å…¶ä»–æ–‡ä»¶ä¸æ˜¯ä¾èµ–æ–‡ä»¶
            return False
        except ValueError:
            # ä¸åœ¨ content/ ç›®å½•ä¸‹ï¼Œè§†ä¸ºä¾èµ–æ–‡ä»¶ï¼ˆå¦‚ config.typï¼‰
            return True

    except Exception:
        return True


def find_typ_dependencies(typ_file: Path) -> set[Path]:
    """
    è§£æ .typ æ–‡ä»¶ä¸­çš„ä¾èµ–ï¼ˆé€šè¿‡ #import å’Œ #include å¯¼å…¥çš„æ–‡ä»¶ï¼‰ã€‚

    åªè¿½è¸ª .typ æ–‡ä»¶çš„ä¾èµ–ï¼Œå¿½ç•¥ content/ ä¸‹çš„æ™®é€šé¡µé¢æ–‡ä»¶ã€‚
    å…¶ä»–èµ„æºæ–‡ä»¶ï¼ˆå¦‚ .md, .bib, å›¾ç‰‡ç­‰ï¼‰é€šè¿‡ copy_content_assets å¤„ç†ã€‚

    å‚æ•°:
        typ_file: .typ æ–‡ä»¶è·¯å¾„

    è¿”å›:
        set[Path]: ä¾èµ–çš„ .typ æ–‡ä»¶è·¯å¾„é›†åˆ
    """
    dependencies: set[Path] = set()

    try:
        content = typ_file.read_text(encoding="utf-8")
    except Exception:
        return dependencies

    # è·å–æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œç”¨äºè§£æç›¸å¯¹è·¯å¾„
    base_dir = typ_file.parent

    patterns = [
        r'#import\s+"([^"]+)"',
        r"#import\s+'([^']+)'",
        r'#include\s+"([^"]+)"',
        r"#include\s+'([^']+)'",
    ]

    for pattern in patterns:
        for match in re.finditer(pattern, content):
            dep_path_str = match.group(1)

            # è·³è¿‡åŒ…å¯¼å…¥ï¼ˆå¦‚ @preview/xxxï¼‰
            if dep_path_str.startswith("@"):
                continue

            # è§£æç›¸å¯¹è·¯å¾„
            if dep_path_str.startswith("/"):
                # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
                dep_path = Path(dep_path_str.lstrip("/"))
            else:
                # ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„è·¯å¾„
                dep_path = base_dir / dep_path_str

            # è§„èŒƒåŒ–è·¯å¾„ï¼Œåªè¿½è¸ª .typ æ–‡ä»¶
            try:
                dep_path = dep_path.resolve()
                if dep_path.exists() and dep_path.suffix == ".typ" and is_dep_file(dep_path):
                    dependencies.add(dep_path)
            except Exception:
                pass

    return dependencies


def get_all_dependencies(typ_file: Path, visited: set[Path] | None = None) -> set[Path]:
    """
    é€’å½’è·å– .typ æ–‡ä»¶çš„æ‰€æœ‰ä¾èµ–ï¼ˆåŒ…æ‹¬ä¼ é€’ä¾èµ–ï¼‰ã€‚

    å‚æ•°:
        typ_file: .typ æ–‡ä»¶è·¯å¾„
        visited: å·²è®¿é—®çš„æ–‡ä»¶é›†åˆï¼ˆç”¨äºé¿å…å¾ªç¯ä¾èµ–ï¼‰

    è¿”å›:
        set[Path]: æ‰€æœ‰ä¾èµ–æ–‡ä»¶è·¯å¾„é›†åˆ
    """
    if visited is None:
        visited = set()

    # é¿å…å¾ªç¯ä¾èµ–
    abs_path = typ_file.resolve()
    if abs_path in visited:
        return set()
    visited.add(abs_path)

    all_deps: set[Path] = set()
    direct_deps = find_typ_dependencies(typ_file)

    for dep in direct_deps:
        all_deps.add(dep)
        # åªå¯¹ .typ æ–‡ä»¶é€’å½’æŸ¥æ‰¾ä¾èµ–
        if dep.suffix == ".typ":
            all_deps.update(get_all_dependencies(dep, visited))

    return all_deps


def needs_rebuild(source: Path, target: Path, extra_deps: list[Path] | None = None) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°æ„å»ºã€‚

    å½“ä»¥ä¸‹ä»»ä¸€æ¡ä»¶æ»¡è¶³æ—¶éœ€è¦é‡å»ºï¼š
    1. ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨
    2. æºæ–‡ä»¶æ¯”ç›®æ ‡æ–‡ä»¶æ–°
    3. ä»»ä½•é¢å¤–ä¾èµ–æ–‡ä»¶æ¯”ç›®æ ‡æ–‡ä»¶æ–°
    4. æºæ–‡ä»¶çš„ä»»ä½•å¯¼å…¥ä¾èµ–æ¯”ç›®æ ‡æ–‡ä»¶æ–°
    5. æºæ–‡ä»¶åŒç›®å½•ä¸‹çš„ä»»ä½•é .typ æ–‡ä»¶æ¯”ç›®æ ‡æ–‡ä»¶æ–°ï¼ˆå¦‚ .md, .bib, å›¾ç‰‡ç­‰ï¼‰

    å‚æ•°:
        source: æºæ–‡ä»¶è·¯å¾„
        target: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        extra_deps: é¢å¤–çš„ä¾èµ–æ–‡ä»¶åˆ—è¡¨ï¼ˆå¦‚ config.typï¼‰

    è¿”å›:
        bool: æ˜¯å¦éœ€è¦é‡æ–°æ„å»º
    """
    # ç›®æ ‡ä¸å­˜åœ¨ï¼Œéœ€è¦æ„å»º
    if not target.exists():
        return True

    target_mtime = get_file_mtime(target)

    # æºæ–‡ä»¶æ›´æ–°äº†
    if get_file_mtime(source) > target_mtime:
        return True

    # æ£€æŸ¥é¢å¤–ä¾èµ–
    if extra_deps:
        for dep in extra_deps:
            if dep.exists() and get_file_mtime(dep) > target_mtime:
                return True

    # æ£€æŸ¥æºæ–‡ä»¶çš„å¯¼å…¥ä¾èµ–
    for dep in get_all_dependencies(source):
        if get_file_mtime(dep) > target_mtime:
            return True

    # æ£€æŸ¥æºæ–‡ä»¶åŒç›®å½•ä¸‹çš„é .typ èµ„æºæ–‡ä»¶ï¼ˆå¦‚ .md, .bib, å›¾ç‰‡ç­‰ï¼‰
    # åªæ£€æŸ¥åŒä¸€ç›®å½•ï¼Œä¸é€’å½’å­ç›®å½•ï¼Œé¿å…è¿‡åº¦é‡ç¼–è¯‘
    source_dir = source.parent
    for item in source_dir.iterdir():
        if item.is_file() and item.suffix != ".typ":
            if get_file_mtime(item) > target_mtime:
                return True

    return False


def find_common_dependencies() -> list[Path]:
    """
    æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶çš„å…¬å…±ä¾èµ–ï¼ˆå¦‚ config.typï¼‰ã€‚

    è¿”å›:
        list[Path]: å…¬å…±ä¾èµ–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    common_deps = []

    # config.typ æ˜¯å…¨å±€é…ç½®ï¼Œä¿®æ”¹åæ‰€æœ‰é¡µé¢éƒ½éœ€è¦é‡å»º
    if CONFIG_FILE.exists():
        common_deps.append(CONFIG_FILE)

    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–å…¬å…±ä¾èµ–
    # ä¾‹å¦‚ï¼šæŸ¥æ‰¾ content/_* ç›®å½•ä¸‹çš„æ¨¡æ¿æ–‡ä»¶
    if CONTENT_DIR.exists():
        for item in CONTENT_DIR.iterdir():
            if item.is_dir() and item.name.startswith("_"):
                for typ_file in item.rglob("*.typ"):
                    common_deps.append(typ_file)

    return common_deps


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================


def find_typ_files() -> list[Path]:
    """
    æŸ¥æ‰¾ content/ ç›®å½•ä¸‹æ‰€æœ‰ .typ æ–‡ä»¶ï¼Œæ’é™¤è·¯å¾„ä¸­åŒ…å«ä»¥ä¸‹åˆ’çº¿å¼€å¤´çš„ç›®å½•çš„æ–‡ä»¶ã€‚

    è¿”å›:
        list[Path]: .typ æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    typ_files = []
    for typ_file in CONTENT_DIR.rglob("*.typ"):
        # æ£€æŸ¥è·¯å¾„ä¸­æ˜¯å¦æœ‰ä»¥ä¸‹åˆ’çº¿å¼€å¤´çš„ç›®å½•
        parts = typ_file.relative_to(CONTENT_DIR).parts
        if not any(part.startswith("_") for part in parts):
            typ_files.append(typ_file)
    return typ_files


def get_html_output_path(typ_file: Path) -> Path:
    """
    è·å– .typ æ–‡ä»¶å¯¹åº”çš„ HTML è¾“å‡ºè·¯å¾„ã€‚

    å‚æ•°:
        typ_file: .typ æ–‡ä»¶è·¯å¾„ (ç›¸å¯¹äº content/)

    è¿”å›:
        Path: HTML æ–‡ä»¶è¾“å‡ºè·¯å¾„ (åœ¨ _site/ ç›®å½•ä¸‹)
    """
    relative_path = typ_file.relative_to(CONTENT_DIR)
    return SITE_DIR / relative_path.with_suffix(".html")


def get_pdf_output_path(typ_file: Path) -> Path:
    """
    è·å– .typ æ–‡ä»¶å¯¹åº”çš„ PDF è¾“å‡ºè·¯å¾„ã€‚

    å‚æ•°:
        typ_file: .typ æ–‡ä»¶è·¯å¾„ (ç›¸å¯¹äº content/)

    è¿”å›:
        Path: PDF æ–‡ä»¶è¾“å‡ºè·¯å¾„ (åœ¨ _site/ ç›®å½•ä¸‹)
    """
    relative_path = typ_file.relative_to(CONTENT_DIR)
    return SITE_DIR / relative_path.with_suffix(".pdf")


def run_typst_command(args: list[str]) -> bool:
    """
    è¿è¡Œ typst å‘½ä»¤ã€‚

    å‚æ•°:
        args: typst å‘½ä»¤å‚æ•°åˆ—è¡¨

    è¿”å›:
        bool: å‘½ä»¤æ˜¯å¦æˆåŠŸæ‰§è¡Œ
    """
    try:
        result = subprocess.run(["typst"] + args, capture_output=True, text=True, encoding="utf-8")
        if result.returncode != 0:
            print(f"  âŒ Typst é”™è¯¯: {result.stderr.strip()}")
            return False
        return True
    except FileNotFoundError:
        print("  âŒ é”™è¯¯: æœªæ‰¾åˆ° typst å‘½ä»¤ã€‚è¯·ç¡®ä¿å·²å®‰è£… Typst å¹¶æ·»åŠ åˆ° PATH ç¯å¢ƒå˜é‡ä¸­ã€‚")
        print("  ğŸ“ å®‰è£…è¯´æ˜: https://typst.app/open-source/#download")
        return False
    except Exception as e:
        print(f"  âŒ æ‰§è¡Œ typst å‘½ä»¤æ—¶å‡ºé”™: {e}")
        return False


# ============================================================================
# æ„å»ºå‘½ä»¤
# ============================================================================


def _compile_files(
    files: list[Path],
    force: bool,
    common_deps: list[Path],
    get_output_path_func,
    build_args_func,
) -> BuildStats:
    """
    é€šç”¨æ–‡ä»¶ç¼–è¯‘å‡½æ•°ï¼Œå‡å°‘é‡å¤ä»£ç ã€‚

    å‚æ•°:
        files: è¦ç¼–è¯‘çš„æ–‡ä»¶åˆ—è¡¨
        force: æ˜¯å¦å¼ºåˆ¶é‡å»º
        common_deps: å…¬å…±ä¾èµ–åˆ—è¡¨
        get_output_path_func: è·å–è¾“å‡ºè·¯å¾„çš„å‡½æ•°
        build_args_func: æ„å»ºç¼–è¯‘å‚æ•°çš„å‡½æ•°

    è¿”å›:
        BuildStats: æ„å»ºç»Ÿè®¡ä¿¡æ¯
    """
    stats = BuildStats()

    for typ_file in files:
        output_path = get_output_path_func(typ_file)

        # å¢é‡ç¼–è¯‘æ£€æŸ¥
        if not force and not needs_rebuild(typ_file, output_path, common_deps):
            stats.skipped += 1
            continue

        output_path.parent.mkdir(parents=True, exist_ok=True)

        # æ„å»ºç¼–è¯‘å‚æ•°
        args = build_args_func(typ_file, output_path)

        if run_typst_command(args):
            stats.success += 1
        else:
            print(f"  âŒ {typ_file} ç¼–è¯‘å¤±è´¥")
            stats.failed += 1

    return stats


def build_html(force: bool = False) -> bool:
    """
    ç¼–è¯‘æ‰€æœ‰ .typ æ–‡ä»¶ä¸º HTMLï¼ˆæ–‡ä»¶åä¸­åŒ…å« PDF çš„é™¤å¤–ï¼‰ã€‚

    å‚æ•°:
        force: æ˜¯å¦å¼ºåˆ¶é‡å»ºæ‰€æœ‰æ–‡ä»¶
    """
    typ_files = find_typ_files()

    # æ’é™¤æ ‡è®°ä¸º PDF çš„æ–‡ä»¶
    html_files = [f for f in typ_files if "pdf" not in f.stem.lower()]

    if not html_files:
        print("  âš ï¸ æœªæ‰¾åˆ°ä»»ä½• HTML æ–‡ä»¶ã€‚")
        return True

    print("æ­£åœ¨æ„å»º HTML æ–‡ä»¶...")

    # è·å–å…¬å…±ä¾èµ–
    common_deps = find_common_dependencies()

    def build_html_args(typ_file: Path, output_path: Path) -> list[str]:
        """æ„å»º HTML ç¼–è¯‘å‚æ•°"""
        try:
            rel_path = typ_file.relative_to(CONTENT_DIR)

            if rel_path.name == "index.typ":
                # index.typ uses the parent directory name as the path
                # content/Blog/index.typ -> "Blog"
                # content/index.typ -> "" (Homepage)
                page_path = rel_path.parent.as_posix()
                if page_path == ".":
                    page_path = ""
            else:
                # Common files use the filename as the path
                # content/about.typ -> "about"
                page_path = rel_path.with_suffix("").as_posix()
        except ValueError:
            page_path = ""

        return [
            "compile",
            "--root",
            ".",
            "--font-path",
            str(ASSETS_DIR),
            "--features",
            "html",
            "--format",
            "html",
            "--input",
            f"page-path={page_path}",
            str(typ_file),
            str(output_path),
        ]

    stats = _compile_files(
        html_files,
        force,
        common_deps,
        get_html_output_path,
        build_html_args,
    )

    print(f"âœ… HTML æ„å»ºå®Œæˆã€‚{stats.format_summary()}")
    return not stats.has_failures


def build_pdf(force: bool = False) -> bool:
    """
    ç¼–è¯‘æ–‡ä»¶ååŒ…å« "PDF" çš„ .typ æ–‡ä»¶ä¸º PDFã€‚

    å‚æ•°:
        force: æ˜¯å¦å¼ºåˆ¶é‡å»ºæ‰€æœ‰æ–‡ä»¶
    """
    typ_files = find_typ_files()
    pdf_files = [f for f in typ_files if "pdf" in f.stem.lower()]

    if not pdf_files:
        return True

    print("æ­£åœ¨æ„å»º PDF æ–‡ä»¶...")

    # è·å–å…¬å…±ä¾èµ–
    common_deps = find_common_dependencies()

    def build_pdf_args(typ_file: Path, output_path: Path) -> list[str]:
        """æ„å»º PDF ç¼–è¯‘å‚æ•°"""
        return [
            "compile",
            "--root",
            ".",
            "--font-path",
            str(ASSETS_DIR),
            str(typ_file),
            str(output_path),
        ]

    stats = _compile_files(
        pdf_files,
        force,
        common_deps,
        get_pdf_output_path,
        build_pdf_args,
    )

    print(f"âœ… PDF æ„å»ºå®Œæˆã€‚{stats.format_summary()}")
    return not stats.has_failures


def copy_assets() -> bool:
    """
    å¤åˆ¶é™æ€èµ„æºåˆ°è¾“å‡ºç›®å½•ã€‚
    """
    if not ASSETS_DIR.exists():
        print(f"  âš  é™æ€èµ„æºç›®å½• {ASSETS_DIR} ä¸å­˜åœ¨ã€‚")
        return True

    target_dir = SITE_DIR / "assets"

    try:
        if target_dir.exists():
            shutil.rmtree(target_dir)
        shutil.copytree(ASSETS_DIR, target_dir)
        return True
    except Exception as e:
        print(f"  âŒ å¤åˆ¶é™æ€èµ„æºå¤±è´¥: {e}")
        return False


def copy_content_assets(force: bool = False) -> bool:
    """
    å¤åˆ¶ content ç›®å½•ä¸‹çš„é .typ æ–‡ä»¶ï¼ˆå¦‚å›¾ç‰‡ï¼‰åˆ°è¾“å‡ºç›®å½•ã€‚
    æ”¯æŒå¢é‡å¤åˆ¶ï¼šåªå¤åˆ¶ä¿®æ”¹è¿‡çš„æ–‡ä»¶ã€‚

    å‚æ•°:
        force: æ˜¯å¦å¼ºåˆ¶å¤åˆ¶æ‰€æœ‰æ–‡ä»¶
    """
    if not CONTENT_DIR.exists():
        print(f"  âš  å†…å®¹ç›®å½• {CONTENT_DIR} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
        return True

    try:
        copy_count = 0
        skip_count = 0

        for item in CONTENT_DIR.rglob("*"):
            # è·³è¿‡ç›®å½•å’Œ .typ æ–‡ä»¶
            if item.is_dir() or item.suffix == ".typ":
                continue

            # è·³è¿‡ä»¥ä¸‹åˆ’çº¿å¼€å¤´çš„è·¯å¾„
            relative_path = item.relative_to(CONTENT_DIR)
            if any(part.startswith("_") for part in relative_path.parts):
                continue

            # è®¡ç®—ç›®æ ‡è·¯å¾„
            target_path = SITE_DIR / relative_path

            # å¢é‡å¤åˆ¶æ£€æŸ¥
            if not force and target_path.exists():
                if get_file_mtime(item) <= get_file_mtime(target_path):
                    skip_count += 1
                    continue

            # åˆ›å»ºç›®æ ‡ç›®å½•
            target_path.parent.mkdir(parents=True, exist_ok=True)

            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(item, target_path)
            copy_count += 1

        return True
    except Exception as e:
        print(f"  âŒ å¤åˆ¶å†…å®¹èµ„æºæ–‡ä»¶å¤±è´¥: {e}")
        return False


def clean() -> bool:
    """
    æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶ã€‚
    """
    print("æ­£åœ¨æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶...")

    if not SITE_DIR.exists():
        print(f"  è¾“å‡ºç›®å½• {SITE_DIR} ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†ã€‚")
        return True

    try:
        # åˆ é™¤ _site ç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹
        for item in SITE_DIR.iterdir():
            if item.is_dir():
                shutil.rmtree(item)
            else:
                item.unlink()

        print(f"  âœ… å·²æ¸…ç† {SITE_DIR}/ ç›®å½•ã€‚")
        return True
    except Exception as e:
        print(f"  âŒ æ¸…ç†å¤±è´¥: {e}")
        return False


def preview(port: int = 8000, open_browser_flag: bool = True) -> bool:
    """
    å¯åŠ¨æœ¬åœ°é¢„è§ˆæœåŠ¡å™¨ã€‚

    é¦–å…ˆå°è¯•ä½¿ç”¨ uvx livereloadï¼ˆæ”¯æŒå®æ—¶åˆ·æ–°ï¼‰ï¼Œ
    å¦‚æœå¤±è´¥åˆ™å›é€€åˆ° Python å†…ç½®çš„ http.serverã€‚

    å‚æ•°:
        port: æœåŠ¡å™¨ç«¯å£å·ï¼Œé»˜è®¤ä¸º 8000
        open_browser_flag: æ˜¯å¦è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼Œé»˜è®¤ä¸º True
    """
    if not SITE_DIR.exists():
        print(f"  âš  è¾“å‡ºç›®å½• {SITE_DIR} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ build å‘½ä»¤ã€‚")
        return False

    print("æ­£åœ¨å¯åŠ¨æœ¬åœ°é¢„è§ˆæœåŠ¡å™¨ï¼ˆæŒ‰ Ctrl+C åœæ­¢ï¼‰...")
    print()

    if open_browser_flag:

        def open_browser():
            time.sleep(1.5)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
            url = f"http://localhost:{port}"
            print(f"  ğŸš€ æ­£åœ¨æ‰“å¼€æµè§ˆå™¨: {url}")
            webbrowser.open(url)

        # åœ¨åå°çº¿ç¨‹ä¸­æ‰“å¼€æµè§ˆå™¨
        threading.Thread(target=open_browser, daemon=True).start()

    # é¦–å…ˆå°è¯• uvx livereload
    try:
        result = subprocess.run(
            ["uvx", "livereload", str(SITE_DIR), "-p", str(port)],
            check=False,
        )
        return result.returncode == 0
    except FileNotFoundError:
        print("  æœªæ‰¾åˆ° uvï¼Œå°è¯• Python http.server...")
    except KeyboardInterrupt:
        print("\næœåŠ¡å™¨å·²åœæ­¢ã€‚")
        return True

    # å›é€€åˆ° Python http.server
    try:
        print("ä½¿ç”¨ Python å†…ç½® http.server...")
        result = subprocess.run(
            [sys.executable, "-m", "http.server", str(port), "--directory", str(SITE_DIR)],
            check=False,
        )
        return result.returncode == 0
    except KeyboardInterrupt:
        print("\næœåŠ¡å™¨å·²åœæ­¢ã€‚")
        return True
    except Exception as e:
        print(f"  âŒ å¯åŠ¨æœåŠ¡å™¨å¤±è´¥: {e}")
        return False


def get_site_url() -> str:
    """
    ä» config.typ é…ç½®æ–‡ä»¶ä¸­è§£æç«™ç‚¹ URLã€‚
    
    åŠŸèƒ½:
        é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä» config.typ ä¸­æå– site-url å­—æ®µçš„å€¼ã€‚
        å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥ï¼Œåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    
    è¿”å›:
        str: ç«™ç‚¹çš„æ ¹ URLï¼ˆå¦‚ "https://example.com"ï¼‰ï¼Œæœ«å°¾ä¸å¸¦æ–œæ ã€‚
             å¦‚æœæœªé…ç½®æˆ–è§£æå¤±è´¥åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    if not CONFIG_FILE.exists():
        return ""

    try:
        content = CONFIG_FILE.read_text(encoding="utf-8")
        # Look for site-url: "..."
        if match := re.search(r'site-url\s*:\s*"([^"]*)"', content):
            return match.group(1).strip().rstrip("/")
    except Exception as e:
        print(f"âš ï¸ Warning: Failed to parse site-url from config.typ: {e}")

    return ""


def get_feed_config() -> dict:
    """
    ä» config.typ é…ç½®æ–‡ä»¶ä¸­è§£æ RSS Feed è®¢é˜…æºçš„é…ç½®ä¿¡æ¯ã€‚
    
    åŠŸèƒ½:
        è§£æ config.typ ä¸­çš„ feed é…ç½®å—ï¼Œæå– filenameã€limit å’Œ categories å­—æ®µã€‚
        ä½¿ç”¨æ‹¬å·è®¡æ•°æ³•å¤„ç†åµŒå¥—ç»“æ„ï¼Œç¡®ä¿æ­£ç¡®è§£æå¤šè¡Œé…ç½®ã€‚
    
    è¿”å›:
        dict: RSS Feed é…ç½®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
            - filename (str): RSS æ–‡ä»¶åï¼Œé»˜è®¤ä¸º "feed.xml"
            - limit (int | None): é™åˆ¶è¾“å‡ºçš„æ–‡ç« æ•°é‡ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
            - categories (list[str]): è¦åŒ…å«çš„æ–‡ç« åˆ†ç±»åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©ºåˆ—è¡¨
    """
    config = {"filename": "feed.xml", "limit": None, "categories": []}
    if not CONFIG_FILE.exists():
        return config

    try:
        content = CONFIG_FILE.read_text(encoding="utf-8")
        
        match_start = re.search(r"feed:\s*\(", content)
        if match_start:
            start_idx = match_start.end()
            open_parens = 1
            feed_block = ""
            
            # å‘åéå†ï¼Œé€šè¿‡è®¡æ•°æ‹¬å·æ¥å¤„ç†åµŒå¥—ç»“æ„
            for char in content[start_idx:]:
                if char == '(':
                    open_parens += 1
                elif char == ')':
                    open_parens -= 1
                
                if open_parens == 0:
                    break
                feed_block += char
            
            feed_block_clean = re.sub(r"//.*", "", feed_block) 
            # Match filename: "..."
            if fn_match := re.search(r'filename:\s*"([^"]*)"', feed_block_clean):
                config["filename"] = fn_match.group(1).strip()
            
            # Match limit: 20
            if limit_match := re.search(r"limit:\s*(\d+)", feed_block_clean):
                config["limit"] = int(limit_match.group(1))
            
            # Match categories: ("...", "...")
            if cat_match := re.search(r"categories:\s*\(([^)]*)\)", feed_block_clean, re.DOTALL):
                cats = re.findall(r'"([^"]*)"', cat_match.group(1))
                if cats:
                    config["categories"] = cats

    except Exception as e:
        print(f"âš ï¸ Warning: Failed to parse feed config from config.typ: {e}")
        
    return config

def get_site_language() -> str:
    """
    ä» config.typ é…ç½®æ–‡ä»¶ä¸­è§£æç½‘ç«™è¯­è¨€ä»£ç ã€‚
    
    åŠŸèƒ½:
        é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä» config.typ ä¸­æå– lang å­—æ®µçš„å€¼ã€‚
        ç”¨äºè®¾ç½®ç½‘ç«™çš„ä¸»è¦è¯­è¨€ï¼Œå½±å“ HTML lang å±æ€§å’Œ RSS Feedã€‚
    
    è¿”å›:
        str: è¯­è¨€ä»£ç ï¼ˆå¦‚ "zh", "en" ç­‰ï¼‰ï¼Œé»˜è®¤è¿”å› "zh"ã€‚
    """
    if not CONFIG_FILE.exists():
        return "zh"

    try:
        content = CONFIG_FILE.read_text(encoding="utf-8")
        # Look for lang: "..."
        if match := re.search(r'lang\s*:\s*"([^"]*)"', content):
            return match.group(1).strip()
    except Exception as e:
        print(f"âš ï¸ Warning: Failed to parse lang from config.typ: {e}")

    return "zh"

def get_site_title() -> str:
    """
    ä» config.typ é…ç½®æ–‡ä»¶ä¸­è§£æç½‘ç«™æ ‡é¢˜ã€‚
    
    åŠŸèƒ½:
        é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä» config.typ ä¸­æå– title å­—æ®µçš„å€¼ã€‚
        ç½‘ç«™æ ‡é¢˜å°†ç”¨äº RSS Feed çš„ channel titleã€‚
    
    è¿”å›:
        str: ç½‘ç«™æ ‡é¢˜å­—ç¬¦ä¸²ï¼Œé»˜è®¤è¿”å› "Blog"ã€‚
    """
    if not CONFIG_FILE.exists():
        return "Blog"

    try:
        content = CONFIG_FILE.read_text(encoding="utf-8")
        # Look for title: "..."
        if match := re.search(r'title\s*:\s*"([^"]*)"', content):
            return match.group(1).strip()
    except Exception as e:
        print(f"âš ï¸ Warning: Failed to parse title from config.typ: {e}")

    return "Blog"

def get_site_description() -> str:
    """
    ä» config.typ é…ç½®æ–‡ä»¶ä¸­è§£æç½‘ç«™æè¿°ä¿¡æ¯ã€‚
    
    åŠŸèƒ½:
        é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä» config.typ ä¸­æå– description å­—æ®µçš„å€¼ã€‚
        ç½‘ç«™æè¿°å°†ç”¨äº RSS Feed çš„ channel descriptionã€‚
    
    è¿”å›:
        str: ç½‘ç«™æè¿°å­—ç¬¦ä¸²ï¼Œå¦‚æœæœªé…ç½®åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    if not CONFIG_FILE.exists():
        return ""

    try:
        content = CONFIG_FILE.read_text(encoding="utf-8")
        # Look for description: "..."
        if match := re.search(r'description\s*:\s*"([^"]*)"', content):
            return match.group(1).strip()
    except Exception as e:
        print(f"âš ï¸ Warning: Failed to parse description from config.typ: {e}")

    return ""

def generate_sitemap() -> bool:
    """
    Generate sitemap.xml for the website.
    """
    base_url = get_site_url()
    if not base_url:
        print("âš ï¸ è·³è¿‡ Sitemap æ„å»º: config.typ ä¸­æœªé…ç½® 'site-url'ã€‚")
        return True

    sitemap_path = SITE_DIR / "sitemap.xml"
    urls = []

    # Walk through the _site directory
    for file_path in SITE_DIR.rglob("*.html"):
        # Calculate relative path from _site
        rel_path = file_path.relative_to(SITE_DIR).as_posix()

        # Determine URL path
        if rel_path == "index.html":
            url_path = ""
        elif rel_path.endswith("/index.html"):
            url_path = rel_path.removesuffix("index.html")
        elif rel_path.endswith(".html"):
            url_path = rel_path.removesuffix(".html") + "/"
        else:
            url_path = rel_path

        full_url = f"{base_url}/{url_path}"

        # Get last modification time
        mtime = file_path.stat().st_mtime
        lastmod = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d")

        urls.append(f"""  <url>
    <loc>{html.escape(full_url)}</loc>
    <lastmod>{lastmod}</lastmod>
  </url>""")

    newline = "\n"
    sitemap_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
{newline.join(sorted(urls))}
</urlset>"""

    try:
        sitemap_path.write_text(sitemap_content, encoding="utf-8")
        print(f"âœ… Sitemap æ„å»ºå®Œæˆ: åŒ…å« {len(urls)} ä¸ªé¡µé¢")
        return True
    except Exception as e:
        print(f"âŒ Sitemap æ„å»ºå¤±è´¥: {e}")
        return False


def generate_robots_txt() -> bool:
    """
    Generate robots.txt pointing to the sitemap.
    """
    site_url = get_site_url()
    if not site_url:
        return True

    robots_content = f"""User-agent: *
Allow: /

Sitemap: {site_url}/sitemap.xml
"""

    try:
        (SITE_DIR / "robots.txt").write_text(robots_content, encoding="utf-8")
        return True
    except Exception as e:
        print(f"âŒ ç”Ÿæˆ robots.txt å¤±è´¥: {e}")
        return False


def extract_post_metadata(item: Path, index_file: Path) -> tuple[str, str, datetime | None]:
    """
    ä»æ–‡ç« ç›®å½•å’Œ index.typ æ–‡ä»¶ä¸­æå–æ–‡ç« çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚
    
    åŠŸèƒ½:
        æŒ‰ä¼˜å…ˆçº§é¡ºåºæå–æ–‡ç« å…ƒæ•°æ®ï¼š
        1. æ ‡é¢˜ (title): ä» index.typ çš„ title å­—æ®µæˆ–ä¸€çº§æ ‡é¢˜æå–ï¼Œé»˜è®¤ä½¿ç”¨ç›®å½•å
        2. æè¿° (description): ä» index.typ çš„ description å­—æ®µæå–
        3. æ—¥æœŸ (date): ä¾æ¬¡å°è¯•ä»ä»¥ä¸‹æ¥æºè·å–ï¼š
           - index.typ ä¸­çš„ date: datetime(...) è¯­æ³•
           - æ–‡ä»¶å¤¹åä¸­çš„ YYYY-MM-DD æ ¼å¼æ—¥æœŸ
           - æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´æˆ³
    
    å‚æ•°:
        item (Path): æ–‡ç« æ‰€åœ¨çš„ç›®å½•è·¯å¾„
        index_file (Path): æ–‡ç« çš„ index.typ æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        tuple[str, str, datetime | None]: åŒ…å«ä¸‰ä¸ªå…ƒç´ çš„å…ƒç»„ï¼š
            - str: æ–‡ç« æ ‡é¢˜
            - str: æ–‡ç« æè¿°ï¼ˆå¯èƒ½ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
            - datetime | None: æ–‡ç« æ—¥æœŸï¼ˆå¸¦ UTC æ—¶åŒºï¼‰ï¼Œæ— æ³•è·å–æ—¶ä¸º None
    """
    title = item.name
    description = ""
    date_obj = None

    if index_file.exists():
        try:
            content = index_file.read_text(encoding="utf-8")
            # é¢„å¤„ç†ï¼šç§»é™¤æ³¨é‡Š
            content_clean = re.sub(r'/\*[\s\S]*?\*/', '', content)
            content_clean = re.sub(r'//.*', '', content_clean)
            
            # 1. å°è¯•è§£æ date: datetime(...)
            date_block_match = re.search(
                r'date:\s*datetime\s*\((?P<inner>[^)]+)\)', 
                content_clean, 
                re.IGNORECASE | re.DOTALL
            )

            if date_block_match:
                inner_content = date_block_match.group("inner")
                y = re.search(r'year:\s*(\d{4})', inner_content)
                m = re.search(r'month:\s*(\d{1,2})', inner_content)
                d = re.search(r'day:\s*(\d{1,2})', inner_content)
                
                # ä¹Ÿæ”¯æŒä½ç½®å‚æ•° datetime(2024, 10, 30)
                pos_match = re.search(r'(\d{4}),\s*(\d{1,2}),\s*(\d{1,2})', inner_content)
                
                if y and m and d:
                    date_obj = datetime(int(y.group(1)), int(m.group(1)), int(d.group(1)), tzinfo=timezone.utc)
                elif pos_match:
                    date_obj = datetime(int(pos_match.group(1)), int(pos_match.group(2)), int(pos_match.group(3)), tzinfo=timezone.utc)
            
            # 2. åŒ¹é… title: "..." æˆ–ä¸€çº§æ ‡é¢˜
            if title_match := re.search(r'title:\s*"((?:\\.|[^"\\])*)"', content_clean):
                title = title_match.group(1).replace('\\"', '"').replace('\\\\', '\\').strip()
            elif head_match := re.search(r"^=\s+(.+)$", content_clean, re.MULTILINE):
                title = head_match.group(1).strip()
            
            # 3. åŒ¹é… description: "..."
            if desc_match := re.search(r'description:\s*"((?:\\.|[^"\\])*)"', content_clean):
                description = desc_match.group(1).replace('\\"', '"').replace('\\\\', '\\').strip()
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: è§£æ {index_file} æ—¶å‡ºé”™: {e}")

    # 4. å¦‚æœæ²¡æ‰¾åˆ°æ—¥æœŸï¼Œå°è¯•ä»æ–‡ä»¶å¤¹åæå– (YYYY-MM-DD)
    if not date_obj:
        date_match = re.search(r"(\d{4}-\d{2}-\d{2})", item.name)
        if date_match:
            try:
                date_obj = datetime.strptime(date_match.group(1), "%Y-%m-%d").replace(tzinfo=timezone.utc)
            except ValueError:
                pass

    # 5. æœ€åä¿åº•ï¼šä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´
    if not date_obj:
        try:
            # ä¼˜å…ˆä½¿ç”¨ index.typï¼Œå¦‚æœæ²¡æ‰¾åˆ°åˆ™ä½¿ç”¨æ–‡ä»¶å¤¹
            target_path = index_file if index_file.exists() else item
            date_obj = datetime.fromtimestamp(target_path.stat().st_mtime, tz=timezone.utc)
        except Exception:
            pass

    return title, description, date_obj


def collect_posts(categories: list[str]) -> list[dict]:
    """
    ä»æŒ‡å®šçš„åˆ†ç±»ç›®å½•ä¸­æ”¶é›†æ‰€æœ‰æ–‡ç« çš„å…ƒæ•°æ®ã€‚
    
    åŠŸèƒ½:
        éå†æŒ‡å®šåˆ†ç±»ç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•ï¼Œæå–æ¯ä¸ªæ–‡ç« çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚
        åªå¤„ç†ç›®å½•ï¼ˆæ¯ä¸ªç›®å½•ä»£è¡¨ä¸€ç¯‡æ–‡ç« ï¼‰ï¼Œè·³è¿‡æ™®é€šæ–‡ä»¶ã€‚
        å¦‚æœæ— æ³•ç¡®å®šæ–‡ç« æ—¥æœŸï¼Œåˆ™è·³è¿‡è¯¥æ–‡ç« å¹¶è¾“å‡ºè­¦å‘Šã€‚
    
    å‚æ•°:
        categories (list[str]): è¦æ‰«æçš„åˆ†ç±»ç›®å½•åç§°åˆ—è¡¨ï¼ˆå¦‚ ["Blog", "Docs"]ï¼‰
    
    è¿”å›:
        list[dict]: æ–‡ç« æ•°æ®å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä»¥ä¸‹é”®ï¼š
            - title (str): æ–‡ç« æ ‡é¢˜
            - description (str): æ–‡ç« æè¿°
            - category (str): æ–‡ç« æ‰€å±åˆ†ç±»
            - link (str): æ–‡ç« çš„å®Œæ•´ URL
            - date (datetime): æ–‡ç« æ—¥æœŸå¯¹è±¡ï¼ˆå¸¦æ—¶åŒºï¼‰
    """
    BASE_URL = get_site_url()
    posts = []

    for cat in categories:
        cat_dir = CONTENT_DIR / cat
        if not cat_dir.exists():
            continue

        for item in cat_dir.iterdir():
            if not item.is_dir():
                continue

            index_file = item / "index.typ"
            title, description, date_obj = extract_post_metadata(item, index_file)

            if not date_obj:
                print(f"âš ï¸ æ— æ³•ç¡®å®šæ–‡ç«  '{item.name}' çš„æ—¥æœŸï¼Œå·²è·³è¿‡ã€‚")
                continue

            relative_link = f"/{cat}/{item.name}/"
            full_link = f"{BASE_URL}{relative_link}"

            posts.append({
                "title": title,
                "description": description,
                "category": cat,
                "link": full_link,
                "date": date_obj,
            })

    return posts


def build_rss_xml(posts: list[dict], config: dict, lang: str) -> str:
    """
    æ„å»ºç¬¦åˆ RSS 2.0 è§„èŒƒçš„ XML å†…å®¹å­—ç¬¦ä¸²ã€‚
    
    åŠŸèƒ½:
        ä½¿ç”¨ feedgen åº“æ ¹æ®æ–‡ç« æ•°æ®å’Œç«™ç‚¹é…ç½®ç”Ÿæˆå®Œæ•´çš„ RSS Feed XMLã€‚
        æŒ‰æ—¥æœŸé™åºæ’åºæ–‡ç« ï¼Œä½¿ç”¨ feedgen çš„ API è‡ªåŠ¨å¤„ç† XML è½¬ä¹‰å’Œæ ¼å¼åŒ–ã€‚
        æ”¯æŒæ¡ä»¶è¾“å‡º description æ ‡ç­¾ï¼ˆä»…åœ¨æœ‰æè¿°æ—¶è¾“å‡ºï¼‰ã€‚
    
    å‚æ•°:
        posts (list[dict]): æ–‡ç« æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åº”åŒ…å«:
            - title: æ ‡é¢˜
            - description: æè¿°ï¼ˆå¯é€‰ï¼‰
            - link: æ–‡ç« é“¾æ¥
            - date: datetime å¯¹è±¡
            - category: åˆ†ç±»åç§°
        config (dict): ç«™ç‚¹é…ç½®å­—å…¸ï¼Œåº”åŒ…å«:
            - base_url: ç«™ç‚¹æ ¹ URL
            - site_title: ç«™ç‚¹æ ‡é¢˜
            - site_description: ç«™ç‚¹æè¿°
            - rss_filename: RSS æ–‡ä»¶å
        lang (str): è¯­è¨€ä»£ç ï¼ˆå¦‚ "zh", "en"ï¼‰
    
    è¿”å›:
        str: å®Œæ•´çš„ RSS 2.0 XML å­—ç¬¦ä¸²ï¼ŒåŒ…å« XML å£°æ˜å’Œæ‰€æœ‰å¿…è¦çš„å‘½åç©ºé—´ã€‚
    """
    BASE_URL = config["base_url"]
    site_title = config["site_title"]
    site_description = config["site_description"]
    rss_file_name = config["rss_filename"]
    
    # åˆ›å»º FeedGenerator å¯¹è±¡
    fg = FeedGenerator()
    fg.id(BASE_URL)
    fg.title(site_title)
    fg.link(href=BASE_URL, rel='alternate')
    fg.description(site_description)
    fg.language(lang)
    
    # æ·»åŠ è‡ªé“¾æ¥ï¼ˆRSS Feed è‡ªèº«çš„é“¾æ¥ï¼‰
    rss_url = f"{BASE_URL}/{rss_file_name}"
    fg.link(href=rss_url, rel='self', type='application/rss+xml')
    
    # æ·»åŠ æ–‡ç« æ¡ç›®
    for post in posts:
        fe = fg.add_entry()
        fe.id(post["link"])
        fe.title(post["title"])
        fe.link(href=post["link"])
        fe.published(post["date"])
        
        # ä»…åœ¨æœ‰æè¿°æ—¶æ·»åŠ 
        if post["description"]:
            fe.description(post["description"])
        
        # æ·»åŠ åˆ†ç±»ä¿¡æ¯
        fe.category(term=post["category"])
    
    # ç”Ÿæˆ RSS 2.0 æ ¼å¼çš„ XML å­—ç¬¦ä¸²
    rss_content = fg.rss_str(pretty=True).decode('utf-8')
    
    return rss_content


def generate_rss() -> bool:
    """
    ç”Ÿæˆç½‘ç«™çš„ RSS è®¢é˜…æºæ–‡ä»¶ã€‚
    
    åŠŸèƒ½:
        å®Œæ•´çš„ RSS Feed ç”Ÿæˆæµç¨‹ï¼š
        1. æ£€æŸ¥ site-url é…ç½®ï¼ˆå¿…éœ€ï¼‰
        2. ä» config.typ è¯»å– Feed é…ç½®ï¼ˆæ–‡ä»¶åã€é™åˆ¶æ•°é‡ã€åˆ†ç±»ï¼‰
        3. æ”¶é›†æŒ‡å®šåˆ†ç±»ä¸‹çš„æ‰€æœ‰æ–‡ç« å…ƒæ•°æ®
        4. æŒ‰æ—¥æœŸæ’åºå¹¶é™åˆ¶è¾“å‡ºæ•°é‡ï¼ˆå¦‚æœé…ç½®äº† limitï¼‰
        5. æ„å»º RSS XML å¹¶å†™å…¥æ–‡ä»¶
    
    è¿”å›:
        bool: ç”Ÿæˆæ˜¯å¦æˆåŠŸã€‚åœ¨ä»¥ä¸‹æƒ…å†µè¿”å› Trueï¼š
            - æˆåŠŸç”Ÿæˆ RSS æ–‡ä»¶
            - æœªé…ç½® site-urlï¼ˆè·³è¿‡ç”Ÿæˆï¼‰
            - æœªæ‰¾åˆ°ä»»ä½•åˆ†ç±»ç›®å½•ï¼ˆè·³è¿‡ç”Ÿæˆï¼‰
            - æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼ˆç”Ÿæˆç©º Feedï¼‰
          ä»…åœ¨å‘ç”Ÿå¼‚å¸¸æ—¶è¿”å› Falseã€‚
    """
    BASE_URL = get_site_url()
    if not BASE_URL:
        print("âš ï¸ è·³è¿‡ RSS è®¢é˜…æºç”Ÿæˆ: config.typ ä¸­æœªé…ç½® 'site-url'ã€‚")
        return True
    
    feed_config = get_feed_config()
    categories = feed_config["categories"]
    rss_file_name = feed_config["filename"]
    RSS_FILE = SITE_DIR / rss_file_name
    
    if not categories:
        print("âš ï¸ è·³è¿‡ RSS è®¢é˜…æºç”Ÿæˆ: æœªé…ç½®ä»»ä½•åˆ†ç±»ç›®å½•ã€‚")
        return True

    # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªç›®å½•å­˜åœ¨
    if not any((CONTENT_DIR / cat).exists() for cat in categories):
        print("âš ï¸ è·³è¿‡ RSS è®¢é˜…æºç”Ÿæˆ: é…ç½®çš„åˆ†ç±»ç›®å½•éƒ½ä¸å­˜åœ¨ã€‚")
        return True

    print("æ­£åœ¨ç”Ÿæˆ RSS è®¢é˜…æº...")
    
    # æ”¶é›†æ–‡ç« 
    posts = collect_posts(categories)
    
    if not posts:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼ŒRSS è®¢é˜…æºä¸ºç©ºã€‚")
        return True

    # æŒ‰æ—¥æœŸé™åºæ’åº
    posts = sorted(posts, key=lambda x: x["date"], reverse=True)

    # é™åˆ¶è¾“å‡ºæ–‡ç« æ•°é‡
    if feed_config["limit"]:
        posts = posts[:feed_config["limit"]]

    # è·å–é…ç½®ä¿¡æ¯
    lang = get_site_language()
    site_title = get_site_title()
    site_description = get_site_description()
    
    config = {
        "base_url": BASE_URL,
        "site_title": site_title,
        "site_description": site_description,
        "rss_filename": rss_file_name,
    }
    
    # æ„å»º RSS XML
    try:
        rss_content = build_rss_xml(posts, config, lang)
        RSS_FILE.write_text(rss_content, encoding="utf-8")
        print(f"  âœ… RSS è®¢é˜…æºç”ŸæˆæˆåŠŸ: {RSS_FILE} ({len(posts)} ç¯‡æ–‡ç« )")
        return True
    except ValueError as e:
        print(f"âŒ é”™è¯¯: RSS è®¢é˜…æºç”Ÿæˆå¤±è´¥")
        print(f"   åŸå› : feedgen åº“æŠ¥é”™ - {e}")
        print("   è§£å†³: è¯·æ£€æŸ¥ config.typ ä¸­çš„å¿…éœ€é…ç½®å­—æ®µï¼ˆtitle å’Œ descriptionï¼‰")
        return False
    except Exception as e:
        print(f"âŒ é”™è¯¯: ç”Ÿæˆ RSS è®¢é˜…æºæ—¶å‡ºé”™")
        print(f"   å¼‚å¸¸: {type(e).__name__}: {e}")
        return False



def build(force: bool = False) -> bool:
    """
    å®Œæ•´æ„å»ºï¼šHTML + PDF + èµ„æºã€‚

    å‚æ•°:
        force: æ˜¯å¦å¼ºåˆ¶é‡å»ºæ‰€æœ‰æ–‡ä»¶
    """
    print("-" * 60)
    if force:
        clean()
        print("ğŸ› ï¸ å¼€å§‹å®Œæ•´æ„å»º...")
    else:
        print("ğŸš€ å¼€å§‹å¢é‡æ„å»º...")
    print("-" * 60)

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    SITE_DIR.mkdir(parents=True, exist_ok=True)

    results = []

    print()
    results.append(build_html(force))
    results.append(build_pdf(force))
    print()

    results.append(copy_assets())
    results.append(copy_content_assets(force))
    results.append(generate_sitemap())
    results.append(generate_robots_txt())
    results.append(generate_rss())

    print("-" * 60)
    if all(results):
        print("âœ… æ‰€æœ‰æ„å»ºä»»åŠ¡å®Œæˆï¼")
        print(f"  ğŸ“‚ è¾“å‡ºç›®å½•: {SITE_DIR.absolute()}")
    else:
        print("âš  æ„å»ºå®Œæˆï¼Œä½†æœ‰éƒ¨åˆ†ä»»åŠ¡å¤±è´¥ã€‚")
    print("-" * 60)

    return all(results)


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================


def create_parser() -> argparse.ArgumentParser:
    """
    åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ã€‚
    """
    parser = argparse.ArgumentParser(
        prog="build.py",
        description="Tufted Blog Template æ„å»ºè„šæœ¬ - å°† content ä¸­çš„ Typst æ–‡ä»¶ç¼–è¯‘ä¸º HTML å’Œ PDF",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ„å»ºè„šæœ¬é»˜è®¤åªé‡æ–°ç¼–è¯‘ä¿®æ”¹è¿‡çš„æ–‡ä»¶ï¼Œå¯ä½¿ç”¨ -f/--force é€‰é¡¹å¼ºåˆ¶å®Œæ•´é‡å»ºï¼š
    uv run build.py build --force
    æˆ– python build.py build -f

ä½¿ç”¨ preview å‘½ä»¤å¯åŠ¨æœ¬åœ°é¢„è§ˆæœåŠ¡å™¨ï¼š
    uv run build.py preview
    æˆ– python build.py preview -p 3000  # ä½¿ç”¨è‡ªå®šä¹‰ç«¯å£

æ›´å¤šä¿¡æ¯è¯·å‚é˜… README.md
""",
    )

    subparsers = parser.add_subparsers(dest="command", title="å¯ç”¨å‘½ä»¤", metavar="<command>")

    build_parser = subparsers.add_parser("build", help="å®Œæ•´æ„å»º (HTML + PDF + èµ„æº)")
    build_parser.add_argument("-f", "--force", action="store_true", help="å¼ºåˆ¶å®Œæ•´é‡å»º")

    html_parser = subparsers.add_parser("html", help="ä»…æ„å»º HTML æ–‡ä»¶")
    html_parser.add_argument("-f", "--force", action="store_true", help="å¼ºåˆ¶å®Œæ•´é‡å»º")

    pdf_parser = subparsers.add_parser("pdf", help="ä»…æ„å»º PDF æ–‡ä»¶")
    pdf_parser.add_argument("-f", "--force", action="store_true", help="å¼ºåˆ¶å®Œæ•´é‡å»º")

    subparsers.add_parser("assets", help="ä»…å¤åˆ¶é™æ€èµ„æº")
    subparsers.add_parser("clean", help="æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶")

    preview_parser = subparsers.add_parser("preview", help="å¯åŠ¨æœ¬åœ°é¢„è§ˆæœåŠ¡å™¨")
    preview_parser.add_argument(
        "-p", "--port", type=int, default=8000, help="æœåŠ¡å™¨ç«¯å£å·ï¼ˆé»˜è®¤: 8000ï¼‰"
    )
    preview_parser.add_argument(
        "--no-open", action="store_false", dest="open_browser", help="ä¸è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨"
    )
    preview_parser.set_defaults(open_browser=True)

    return parser


if __name__ == "__main__":
    parser = create_parser()
    args = parser.parse_args()

    if args.command is None:
        parser.print_help()
        sys.exit(0)

    # ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
    script_dir = Path(__file__).parent.absolute()
    os.chdir(script_dir)

    # è·å– force å‚æ•°
    force = getattr(args, "force", False)

    # ä½¿ç”¨ match-case æ‰§è¡Œå¯¹åº”çš„å‘½ä»¤
    match args.command:
        case "build":
            success = build(force)
        case "html":
            SITE_DIR.mkdir(parents=True, exist_ok=True)
            success = build_html(force)
        case "pdf":
            SITE_DIR.mkdir(parents=True, exist_ok=True)
            success = build_pdf(force)
        case "assets":
            SITE_DIR.mkdir(parents=True, exist_ok=True)
            success = copy_assets()
        case "clean":
            success = clean()
        case "preview":
            success = preview(getattr(args, "port", 8000), getattr(args, "open_browser", True))
        case _:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {args.command}")
            success = False

    sys.exit(0 if success else 1)
